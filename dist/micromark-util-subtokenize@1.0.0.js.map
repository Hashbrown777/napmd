{"version":3,"file":"micromark-util-subtokenize@1.0.0.js","sources":["../node_modules/micromark-util-subtokenize/index.js"],"sourcesContent":["/**\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').Chunk} Chunk\n * @typedef {import('micromark-util-types').Event} Event\n */\nimport {splice} from 'micromark-util-chunked'\n\n/**\n * Tokenize subcontent.\n *\n * @param {Event[]} events\n * @returns {boolean}\n */\nexport function subtokenize(events) {\n  /** @type {Record<string, number>} */\n  const jumps = {}\n  let index = -1\n  /** @type {Event} */\n\n  let event\n  /** @type {number|undefined} */\n\n  let lineIndex\n  /** @type {number} */\n\n  let otherIndex\n  /** @type {Event} */\n\n  let otherEvent\n  /** @type {Event[]} */\n\n  let parameters\n  /** @type {Event[]} */\n\n  let subevents\n  /** @type {boolean|undefined} */\n\n  let more\n\n  while (++index < events.length) {\n    while (index in jumps) {\n      index = jumps[index]\n    }\n\n    event = events[index] // Add a hook for the GFM tasklist extension, which needs to know if text\n    // is in the first content of a list item.\n\n    if (\n      index &&\n      event[1].type === 'chunkFlow' &&\n      events[index - 1][1].type === 'listItemPrefix'\n    ) {\n      subevents = event[1]._tokenizer.events\n      otherIndex = 0\n\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'lineEndingBlank'\n      ) {\n        otherIndex += 2\n      }\n\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'content'\n      ) {\n        while (++otherIndex < subevents.length) {\n          if (subevents[otherIndex][1].type === 'content') {\n            break\n          }\n\n          if (subevents[otherIndex][1].type === 'chunkText') {\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true\n            otherIndex++\n          }\n        }\n      }\n    } // Enter.\n\n    if (event[0] === 'enter') {\n      if (event[1].contentType) {\n        Object.assign(jumps, subcontent(events, index))\n        index = jumps[index]\n        more = true\n      }\n    } // Exit.\n    else if (event[1]._container) {\n      otherIndex = index\n      lineIndex = undefined\n\n      while (otherIndex--) {\n        otherEvent = events[otherIndex]\n\n        if (\n          otherEvent[1].type === 'lineEnding' ||\n          otherEvent[1].type === 'lineEndingBlank'\n        ) {\n          if (otherEvent[0] === 'enter') {\n            if (lineIndex) {\n              events[lineIndex][1].type = 'lineEndingBlank'\n            }\n\n            otherEvent[1].type = 'lineEnding'\n            lineIndex = otherIndex\n          }\n        } else {\n          break\n        }\n      }\n\n      if (lineIndex) {\n        // Fix position.\n        event[1].end = Object.assign({}, events[lineIndex][1].start) // Switch container exit w/ line endings.\n\n        parameters = events.slice(lineIndex, index)\n        parameters.unshift(event)\n        splice(events, lineIndex, index - lineIndex + 1, parameters)\n      }\n    }\n  }\n\n  return !more\n}\n/**\n * Tokenize embedded tokens.\n *\n * @param {Event[]} events\n * @param {number} eventIndex\n * @returns {Record<string, number>}\n */\n\nfunction subcontent(events, eventIndex) {\n  const token = events[eventIndex][1]\n  const context = events[eventIndex][2]\n  let startPosition = eventIndex - 1\n  /** @type {number[]} */\n\n  const startPositions = []\n  const tokenizer =\n    token._tokenizer || context.parser[token.contentType](token.start)\n  const childEvents = tokenizer.events\n  /** @type {[number, number][]} */\n\n  const jumps = []\n  /** @type {Record<string, number>} */\n\n  const gaps = {}\n  /** @type {Chunk[]} */\n\n  let stream\n  /** @type {Token|undefined} */\n\n  let previous\n  let index = -1\n  /** @type {Token|undefined} */\n\n  let current = token\n  let adjust = 0\n  let start = 0\n  const breaks = [start] // Loop forward through the linked tokens to pass them in order to the\n  // subtokenizer.\n\n  while (current) {\n    // Find the position of the event for this token.\n    while (events[++startPosition][1] !== current) {\n      // Empty.\n    }\n\n    startPositions.push(startPosition)\n\n    if (!current._tokenizer) {\n      stream = context.sliceStream(current)\n\n      if (!current.next) {\n        stream.push(null)\n      }\n\n      if (previous) {\n        tokenizer.defineSkip(current.start)\n      }\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = true\n      }\n\n      tokenizer.write(stream)\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined\n      }\n    } // Unravel the next token.\n\n    previous = current\n    current = current.next\n  } // Now, loop back through all events (and linked tokens), to figure out which\n  // parts belong where.\n\n  current = token\n\n  while (++index < childEvents.length) {\n    if (\n      // Find a void token that includes a break.\n      childEvents[index][0] === 'exit' &&\n      childEvents[index - 1][0] === 'enter' &&\n      childEvents[index][1].type === childEvents[index - 1][1].type &&\n      childEvents[index][1].start.line !== childEvents[index][1].end.line\n    ) {\n      start = index + 1\n      breaks.push(start) // Help GC.\n\n      current._tokenizer = undefined\n      current.previous = undefined\n      current = current.next\n    }\n  } // Help GC.\n\n  tokenizer.events = [] // If there’s one more token (which is the cases for lines that end in an\n  // EOF), that’s perfect: the last point we found starts it.\n  // If there isn’t then make sure any remaining content is added to it.\n\n  if (current) {\n    // Help GC.\n    current._tokenizer = undefined\n    current.previous = undefined\n  } else {\n    breaks.pop()\n  } // Now splice the events from the subtokenizer into the current events,\n  // moving back to front so that splice indices aren’t affected.\n\n  index = breaks.length\n\n  while (index--) {\n    const slice = childEvents.slice(breaks[index], breaks[index + 1])\n    const start = startPositions.pop()\n    jumps.unshift([start, start + slice.length - 1])\n    splice(events, start, 2, slice)\n  }\n\n  index = -1\n\n  while (++index < jumps.length) {\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1]\n    adjust += jumps[index][1] - jumps[index][0] - 1\n  }\n\n  return gaps\n}\n"],"names":["subcontent","events","eventIndex","token","context","startPosition","startPositions","tokenizer","_tokenizer","parser","contentType","start","childEvents","jumps","gaps","stream","previous","index","current","adjust","breaks","push","sliceStream","next","defineSkip","_isInFirstContentOfListItem","_gfmTasklistFirstContentOfListItem","write","undefined","length","type","line","end","pop","slice","unshift","splice","event","lineIndex","otherIndex","otherEvent","parameters","subevents","more","Object","assign","_container"],"mappings":"2DAmIA,SAASA,EAAWC,EAAQC,GAC1B,MAAMC,EAAQF,EAAOC,GAAY,GAC3BE,EAAUH,EAAOC,GAAY,GACnC,IAAIG,EAAgBH,EAAa,EAGjC,MAAMI,EAAiB,GACjBC,EACJJ,EAAMK,YAAcJ,EAAQK,OAAON,EAAMO,aAAaP,EAAMQ,OACxDC,EAAcL,EAAUN,OAGxBY,EAAQ,GAGRC,EAAO,GAGb,IAAIC,EAGAC,EACAC,GAAS,EAGTC,EAAUf,EACVgB,EAAS,EACTR,EAAQ,EACZ,MAAMS,EAAS,CAACT,GAGhB,KAAOO,GAAS,CAEd,KAAOjB,IAASI,GAAe,KAAOa,IAItCZ,EAAee,KAAKhB,GAEfa,EAAQV,aACXO,EAASX,EAAQkB,YAAYJ,GAExBA,EAAQK,MACXR,EAAOM,KAAK,MAGVL,GACFT,EAAUiB,WAAWN,EAAQP,OAG3BO,EAAQO,8BACVlB,EAAUmB,oCAAqC,GAGjDnB,EAAUoB,MAAMZ,GAEZG,EAAQO,8BACVlB,EAAUmB,wCAAqCE,IAInDZ,EAAWE,EACXA,EAAUA,EAAQK,KAMpB,IAFAL,EAAUf,IAEDc,EAAQL,EAAYiB,QAGC,SAA1BjB,EAAYK,GAAO,IACW,UAA9BL,EAAYK,EAAQ,GAAG,IACvBL,EAAYK,GAAO,GAAGa,OAASlB,EAAYK,EAAQ,GAAG,GAAGa,MACzDlB,EAAYK,GAAO,GAAGN,MAAMoB,OAASnB,EAAYK,GAAO,GAAGe,IAAID,OAE/DpB,EAAQM,EAAQ,EAChBG,EAAOC,KAAKV,GAEZO,EAAQV,gBAAaoB,EACrBV,EAAQF,cAAWY,EACnBV,EAAUA,EAAQK,MAmBtB,IAfAhB,EAAUN,OAAS,GAIfiB,GAEFA,EAAQV,gBAAaoB,EACrBV,EAAQF,cAAWY,GAEnBR,EAAOa,MAIThB,EAAQG,EAAOS,OAERZ,KAAS,CACd,MAAMiB,EAAQtB,EAAYsB,MAAMd,EAAOH,GAAQG,EAAOH,EAAQ,IACxDN,EAAQL,EAAe2B,MAC7BpB,EAAMsB,QAAQ,CAACxB,EAAOA,EAAQuB,EAAML,OAAS,IAC7CO,SAAOnC,EAAQU,EAAO,EAAGuB,GAK3B,IAFAjB,GAAS,IAEAA,EAAQJ,EAAMgB,QACrBf,EAAKK,EAASN,EAAMI,GAAO,IAAME,EAASN,EAAMI,GAAO,GACvDE,GAAUN,EAAMI,GAAO,GAAKJ,EAAMI,GAAO,GAAK,EAGhD,OAAOH,gBAxOF,SAAqBb,GAE1B,MAAMY,EAAQ,GACd,IAGIwB,EAGAC,EAGAC,EAGAC,EAGAC,EAGAC,EAGAC,EArBA1B,GAAS,EAuBb,OAASA,EAAQhB,EAAO4B,QAAQ,CAC9B,KAAOZ,KAASJ,GACdI,EAAQJ,EAAMI,GAMhB,GAHAoB,EAAQpC,EAAOgB,GAIbA,GACkB,cAAlBoB,EAAM,GAAGP,MACqB,mBAA9B7B,EAAOgB,EAAQ,GAAG,GAAGa,OAErBY,EAAYL,EAAM,GAAG7B,WAAWP,OAChCsC,EAAa,EAGXA,EAAaG,EAAUb,QACW,oBAAlCa,EAAUH,GAAY,GAAGT,OAEzBS,GAAc,GAIdA,EAAaG,EAAUb,QACW,YAAlCa,EAAUH,GAAY,GAAGT,MAEzB,OAASS,EAAaG,EAAUb,QACQ,YAAlCa,EAAUH,GAAY,GAAGT,MAIS,cAAlCY,EAAUH,GAAY,GAAGT,OAC3BY,EAAUH,GAAY,GAAGd,6BAA8B,EACvDc,KAMR,GAAiB,UAAbF,EAAM,GACJA,EAAM,GAAG3B,cACXkC,OAAOC,OAAOhC,EAAOb,EAAWC,EAAQgB,IACxCA,EAAQJ,EAAMI,GACd0B,GAAO,QAGN,GAAIN,EAAM,GAAGS,WAAY,CAI5B,IAHAP,EAAatB,EACbqB,OAAYV,EAELW,MACLC,EAAavC,EAAOsC,GAGK,eAAvBC,EAAW,GAAGV,MACS,oBAAvBU,EAAW,GAAGV,OAEQ,UAAlBU,EAAW,KACTF,IACFrC,EAAOqC,GAAW,GAAGR,KAAO,mBAG9BU,EAAW,GAAGV,KAAO,aACrBQ,EAAYC,GAOdD,IAEFD,EAAM,GAAGL,IAAMY,OAAOC,OAAO,GAAI5C,EAAOqC,GAAW,GAAG3B,OAEtD8B,EAAaxC,EAAOiC,MAAMI,EAAWrB,GACrCwB,EAAWN,QAAQE,GACnBD,SAAOnC,EAAQqC,EAAWrB,EAAQqB,EAAY,EAAGG,KAKvD,OAAQE"}